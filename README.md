# beginner_to_intermediate_AI

## [approx] Prerequisite Graph
![](images/prereq_graph.png)

## Links
<table class="c8"><tbody><tr class="c6"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">Topic</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">Link</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">Description</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">Prerequisite</span></p></td></tr><tr class="c6"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">1.Probability</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.youtube.com/playlist?list%3DPLUl4u3cNGP61MdtwGTqZA0MreSaDybji8&amp;sa=D&amp;source=editors&amp;ust=1620867886646000&amp;usg=AOvVaw0gm6cr9MGgKQrG_0ysVlko">YT playlist</a></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">MIT 6.041 Probabilistic Systems Analysis and Applied Probability, Fall 2010</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">taken for granted</span></p></td></tr><tr class="c6"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">2.Statistics</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.youtube.com/playlist?list%3DPLUl4u3cNGP60uVBMaoNERc6knT_MgPKS0&amp;sa=D&amp;source=editors&amp;ust=1620867886648000&amp;usg=AOvVaw3ze6WNf0SqlTltzvOX6ttw">YT playlist</a></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">MIT 18.650 Statistics for Applications, Fall 2016</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">Probability</span></p></td></tr><tr class="c6"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">4.Hidden Markov Model</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DjY2E6ExLxaw&amp;sa=D&amp;source=editors&amp;ust=1620867886650000&amp;usg=AOvVaw1H9yu6uFvaKGKXpIyc5r6v">YT lecture</a></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">A lecture of Undergraduate Machine Learning at UBC by Nando de Freitas, 2012</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">Probability: Bayes Theorem</span></p></td></tr><tr class="c6"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">6.Variational Bayes</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.youtube.com/playlist?list%3DPLxC_ffO4q_rW_8Fflob7SuMwpjlSh5f5K&amp;sa=D&amp;source=editors&amp;ust=1620867886652000&amp;usg=AOvVaw0p40EM-xt84FI4fwASYG-z">YT playlist</a></span></p><p class="c5"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://people.csail.mit.edu/tbroderick/tutorials.html&amp;sa=D&amp;source=editors&amp;ust=1620867886652000&amp;usg=AOvVaw1G8C9sW6G5oUJ-BMhm9zI2">Slides</a></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">Lecture series of Tamara Broderick from MIT</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">Probability: Bayesian inference</span></p><p class="c5"><span class="c3">Statistics: KL divergence(optional)</span></p></td></tr><tr class="c6"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">7.Variational Autoencoder</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DuaaqyVS9-rM&amp;sa=D&amp;source=editors&amp;ust=1620867886654000&amp;usg=AOvVaw2xgnUgkUfxG5YTwYQXDBE4">YT lecture</a></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">Ali Ghodsi Deep Learning lecture, University of Waterloo</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">Variational Bayes, KL divergence</span></p></td></tr><tr class="c6"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">3.Reinforcement Learning</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DO5hkIfUR_uA&amp;sa=D&amp;source=editors&amp;ust=1620867886656000&amp;usg=AOvVaw2vzLiN3BIF-TH3OJ6Cau_i">State machines and Markov decision processes</a></span></p><p class="c5 c7"><span class="c3"></span></p><p class="c5"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DIr93m9OmOHo&amp;sa=D&amp;source=editors&amp;ust=1620867886656000&amp;usg=AOvVaw29TEwakrghctiB4DFY6niA">Reinforcement learning</a></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">MIT 6.036 Machine Learning Fall 2020, by Tamara Broderick</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">Taken for granted</span></p></td></tr><tr class="c6"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">8.Attention, Transformer</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DOyFJWRnt_AY&amp;sa=D&amp;source=editors&amp;ust=1620867886658000&amp;usg=AOvVaw00zUbVzrurngVBEtI4MvNB">YT lecture</a></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">CS 460/680, Intro to ML, Pascal Poupart</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c3">Taken for granted</span></p></td></tr></tbody></table>