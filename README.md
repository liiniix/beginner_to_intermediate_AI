# beginner_to_intermediate_AI

## [approx] Prerequisite Graph
![](images/prereq_graph.png)

## Links
<table class="c4"><tbody><tr class="c6"><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">Topic</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">Link</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">Description</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">Prerequisite</sspan></p></td></tr><tr class="c6"><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">1.Probability</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c2"><a class="c5" href="https://www.google.com/url?q=https://www.youtube.com/playlist?list%3DPLUl4u3cNGP61MdtwGTqZA0MreSaDybji8&amp;sa=D&amp;source=editors&amp;ust=1621135918997000&amp;usg=AOvVaw3AhdGz2hXV2Zx7UsOz4deV">YT playlist</a></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">MIT 6.041 Probabilistic Systems Analysis and Applied Probability, Fall 2010</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">taken for granted</span></p></td></tr><tr class="c6"><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">2.Statistics</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c2"><a class="c5" href="https://www.google.com/url?q=https://www.youtube.com/playlist?list%3DPLUl4u3cNGP60uVBMaoNERc6knT_MgPKS0&amp;sa=D&amp;source=editors&amp;ust=1621135918998000&amp;usg=AOvVaw1S_BT_tU6kcgr4VHJf-LmA">YT playlist</a></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">MIT 18.650 Statistics for Applications, Fall 2016</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">Probability</span></p></td></tr><tr class="c6"><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">4.Hidden Markov Model</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c2"><a class="c5" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DjY2E6ExLxaw&amp;sa=D&amp;source=editors&amp;ust=1621135919000000&amp;usg=AOvVaw1nHwB_3tKPsF-Ngrwa4YfW">YT lecture</a></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">A lecture of Undergraduate Machine Learning at UBC by Nando de Freitas, 2012</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">Probability: Bayes Theorem</span></p></td></tr><tr class="c6"><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">6.Variational Bayes</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c2"><a class="c5" href="https://www.google.com/url?q=https://www.youtube.com/playlist?list%3DPLxC_ffO4q_rW_8Fflob7SuMwpjlSh5f5K&amp;sa=D&amp;source=editors&amp;ust=1621135919001000&amp;usg=AOvVaw1DmF62feFOukts2J2sMTrw">YT playlist</a></span></p><p class="c3"><span class="c2"><a class="c5" href="https://www.google.com/url?q=https://people.csail.mit.edu/tbroderick/tutorials.html&amp;sa=D&amp;source=editors&amp;ust=1621135919002000&amp;usg=AOvVaw0h218JHApm9oft4EDRcMaA">Slides</a></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">Lecture series of Tamara Broderick from MIT</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">Probability: Bayesian inference</span></p><p class="c3"><span class="c1">Statistics: KL divergence(optional)</span></p></td></tr><tr class="c6"><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">7.Variational Autoencoder</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c2"><a class="c5" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DuaaqyVS9-rM&amp;sa=D&amp;source=editors&amp;ust=1621135919003000&amp;usg=AOvVaw1vA7H-UwHZtDTGE4Jwb9GI">YT lecture</a></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">Ali Ghodsi Deep Learning lecture, University of Waterloo</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">Variational Bayes, KL divergence</span></p></td></tr><tr class="c6"><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">3.Reinforcement Learning</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c2"><a class="c5" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DO5hkIfUR_uA&amp;sa=D&amp;source=editors&amp;ust=1621135919005000&amp;usg=AOvVaw0VaewXwcFco130r2Z8f8q9">State machines and Markov decision processes</a></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c2"><a class="c5" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DIr93m9OmOHo&amp;sa=D&amp;source=editors&amp;ust=1621135919005000&amp;usg=AOvVaw27BeKxm-7boRy5ZTJGbBMC">Reinforcement learning</a></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">MIT 6.036 Machine Learning Fall 2020, by Tamara Broderick</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">Taken for granted</span></p></td></tr><tr class="c6"><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">8.Attention, Transformer</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c2"><a class="c5" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DOyFJWRnt_AY&amp;sa=D&amp;source=editors&amp;ust=1621135919006000&amp;usg=AOvVaw2LSAAJlzkcJQZuLO3Af4pv">YT lecture</a></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">CS 460/680, Intro to ML, Pascal Poupart</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">Taken for granted</span></p></td></tr><tr class="c6"><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">9.Information Theory</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c2"><a class="c5" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DVhhNcmPLKgo%26list%3DPLp6ek2hDcoNBtaNoXzFE3BYKNZ2kUJo1f%26index%3D2&amp;sa=D&amp;source=editors&amp;ust=1621135919008000&amp;usg=AOvVaw1NecRbJeD3Kket6GhTg8bn">Introduction to Information Theory</a></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c2"><a class="c5" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3D98BAo8L-o6s%26list%3DPLp6ek2hDcoNBtaNoXzFE3BYKNZ2kUJo1f%26index%3D3&amp;sa=D&amp;source=editors&amp;ust=1621135919009000&amp;usg=AOvVaw15yX3nphNOXkesODg3gWhV">Entropy, Mutual Information, Conditional and Joint Entropy</a></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c2"><a class="c5" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3D-pHUIQ0sDqA&amp;sa=D&amp;source=editors&amp;ust=1621135919009000&amp;usg=AOvVaw0YGOwTBj6NMLNLF_tul03X">Measures for Continuous, Random Variable, Relative Entropy</a></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">First three lectures of Information Theory, Coding and Cryptography, offered by IIT, Delhi</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c3"><span class="c1">Probability</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c1">KL divergence (optional)</span></p></td></tr></tbody></table>